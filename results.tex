This section presents experimental results for our compilation strategy.  We
compare the execution speed of our new compiled code against hand-written
implementations in C of the same programs. We also compare the results against
interpreted execution in order to help us understand the limitations of the
compilation scheme when removing the interpretation overhead.

For our experimental setup, we used a computer with a 24 (4x6) Core AMD
Opteron(tm) Processor 8425 HE $@$ 800 MHz with 64 GBytes of RAM memory running
the Linux kernel 3.15.10-201.fc20.x86\_64. The C++ compiler used is GCC 4.8.3
(g++) with the flags: \texttt{-O3 -std=c+0x -march=x86-64}.  We run all
experiments 3 times and averaged the execution time.

We have implemented 5 different LM programs and their corresponding
C versions. The programs are the following:

\begin{itemize}

\item Shortest Path (SP): a slightly modified version of the program presented
   in Fig.~\ref{fig:shortest_path_program}, where the shortest distance is
   computed from all nodes to all nodes.

\item N-Queens: the classic puzzle for placing queens on a chess board so that
no two queens threaten each other.

\item Belief Propagation: a machine learning algorithm to denoise images.

\item Heat Transfer: an asynchronous program that performs transfer of heat
between nodes.

\item MiniMax: the AI algorithm for selecting the best player move in
   a Tic-Tac-Toe game. The initial board was augmented in order to provide a
   longer running benchmark.

\end{itemize}

Table~\ref{fig:table_results} presents experimental results comparing the
compiled and interpreted code versions against the C program versions.
Comparisons to other systems are shown under the \textbf{Other} column. Note
that for some programs, we present different program sizes shown in ascending
order.

\begin{table}[h!]
\begin{center}
    \begin{tabular}{c || c | c | c | c | c}
    \textbf{Program} & \textbf{Size} & \textbf{C Time} (s) & \textbf{Compiled} & \textbf{Interpreted}
    & \textbf{Other} \\ \hline \hline
    \multirow{3}{*}{Shortest Path} & US Airports & 0.1 & 3.9 & 13.9 & 13.3 (python) \\
                                   & OCLinks & 0.4 & 5.6 & 14.2 & 11.2 (python) \\
                                   & Powergrid & 0.9 & 3.5 & 11.3 & 10.6
                                   (python) \\ \hline
    \multirow{4}{*}{N-Queens} & 11 & 0.2 & 1.4 & 3.9 & 20.8 (python) \\
                              & 12 & 1.3 & 3.2 & 5.3 & 24.1 (python) \\
                              & 13 & 7.8 & 3.8 & 6.6 & 26.0 (python) \\
                              & 14 & 49 & 4.5 & 8.9 & 28.0 (python) \\ \hline
    \multirow{4}{*}{Belief Propagation} & 50 & 2.8 & 1.3 & 1.4 & 1.1 (GL) \\
                                        & 200 & 51 & 1.3 & 1.4 & 1.1 (GL) \\ 
                                        & 300 & 141 & 1.3 & 1.4 & 1.1 (GL) \\
                                        & 400 & 180 & 1.3 & 1.4 & 1.1 (GL) \\
                                        \hline
    \multirow{2}{*}{Heat Transfer} & 80 & 7.3 & 4.6 & 9.9 & - \\
                                   & 120 & 32 & 5.3 & 10.5 & - \\ \hline

  MiniMax & - & 7.3 & 3.2 & 7.1 & 9.3 (python) \\
    \end{tabular}
\end{center}

\caption{Experimental results comparing different programs against hand-written
   versions in C. For the C versions, we show the execution time in seconds
   (column \textbf{C Time} (s)). For the other approaches, we show the overhead
   ratio compared with the corresponding C version.  The overhead numbers
   (\textbf{lower is better}) are computed by dividing the execution time of the
approach on that column by the execution time of the similar hand-written
version in C.}

\label{fig:table_results}
    %\vspace*{-0.7cm}
\end{table}

The Shortest Path program shows good improvements from the interpreted version,
since the run time is reduced between 61\% and 72\%.  The good performance results
come from the fact that the program performs repeated comparisons between
integer numbers, which tend to be slower in interpreted code, and from the fact
that the program has only two rules where the shortest distance fact is updated
or kept.  The distance facts are also indexed by the source node, which helps
the code filter through the candidate distances faster. This is helpful
since the program computes the shortest distance between pairs of nodes.

N-Queens presents some scalability issues for our compilation scheme due to the
exponential increase of facts as the problem size increases.  The same behavior
can be observed for the Python programs. Regarding the comparison with the
interpreted version, the compiled version reduces the interpreted run time by
almost 50\% which indicates that there are more database operations in N-Queens
than in Shortest Path.

The Belief Propagation program is made of many expensive floating point
calculations. The interpreted version used external functions written in C to
implement those operations because otherwise it would be too slow. Therefore,
and since the rules tend to manipulate a small number of facts, the interpreted
and compiled versions perform about the same. This program has also the best
results which proves that the program spends a huge amount of time performing
floating point calculations. For comparison purposes, we used
GraphLab~\cite{GraphLab2010} (GC in the table), an efficient machine learning
framework for writing parallel graph-based machine learning algorithms in C++.
GraphLab's version of the algorithm is slightly slower than the C version.

The Heat Transfer program also performs floating point operations but in a much
smaller scale than Belief Propagation. This is noticiable from the results since
the slowdown is much larger than Belief Propagation. The program also needs to
compute many sum aggregates, which makes the interpreted version incur in some
overhead due to the integer operations.

While all the other programs perform computations on a pre-defined set of nodes,
the MiniMax program creates the nodes of the graph dynamically.  Creating new
nodes requires creating new databases which tends to take a considerable
fraction of the run time. However, we have seen a good reduction in run time
when compared to the interpreted version, which we think is the result of
low-level optimizations that were applied in the compiled version.

It should be noted that in these programs there is a parallelization overhead
since LM's supporting runtime is designed to explore parallelism implicitly. For
instance, we measured a 20\% overhead for N-Queens, a program that needs to
reference count many lists during run time. Fortunately, if the programmer takes
advantage of the parallel facilities of LM, she will be able to run most of
these programs faster than C by using between 2 and 4 threads.


\iffalse
C
sp powergrid: 966
sp oclinks: 422
sp airports: 115
minimax:  7267
queens 11: 238
queens 12: 1320
queens 13: 7837
queens 14: 49258
bp 50: 2822
bp 200: 51506
bp 300: 141000
bp 400: 180000
ht 80: 7291
ht 120: 32916

python
sp powergrid: 10221
sp oclinks: 4726
sp airports: 1525
queens 11: 4964
queens 12: 31792
queens 13: 204000
queens 14: 1379000
minimax: 67486

graphlab:
bp 50: 3110
bp 200: 56742
bp 300: 156457
bp 400: 200000

compiled
queens 11: 345
queens 12: 4314
queens 13: 30385
queens 14: 225885
minimax: 23366
sp powergrid: 3467
sp oclinks: 2364
sp airports: 458
bp 50: 3576
bp 200: 61746
bp 300: 175512
bp 400: 220225
ht 80: 34228
ht 120: 175598

interpreted
sp powergrid: 13439
sp oclinks: 5986
sp airports: 1297
minimax:  51507
queens 11: 941
queens 12: 6969
queens 13: 51806
queens 14: 437493
bp 50: 4157
bp 200:
bp 300:
bp 400:
ht 80: 72360
ht 120: 345930

%%%%% old

interpreted
sp powergrid: 13007
sp oclinks: 5704
sp airports: 1199
minimax: 45207
queens 11: 1124
queens 12: 7925
queens 13: 60221
queens 14: 436541
bp 50: 3826
bp 200: 74407
bp 300: 198903
bp 400: 260063
ht 80: 77277
ht 120: 382982

compiled
sp powergrid: 4075
sp oclinks: 2667
sp airports: 547
minimax: 29616
queens 11: 772
queens 12: 6113
queens 13: 48756
queens 14: 360710
bp 50: 3508
bp 200: 64237
bp 300: 172000
bp 400: 229012
ht 80: 50486
ht 120: 254359

\fi
