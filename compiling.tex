
In this section, we present the main algorithm of the compiler that turns
inference rules into C++ code and also other optimizations to
create efficient code.

\subsection{Compiling Rules}\label{sec:compile}

After an inference rule is compiled, it must respect the \emph{fact constraints} (facts
must exist in the database) and also the \emph{join constraints} represented by
variable constraints and boolean expressions. For instance, in the second rule of the program presented
in Fig.~\ref{code:shortest_path_program}:

\begin{Verbatim}[fontsize=\scriptsize,label=example_rule]
shortest(A, D1, P1), D1 <= D2, relax(A, D2, P2)
   -o shortest(A, D1, P1).
\end{Verbatim}

The fact constraints represent the facts that must exist, namely
\texttt{shortest(A, D1, P1)} and \texttt{relax(A, D2, P2)} and the join
constraints are represented by the constraint \texttt{D1 <= D2}.

However, rules may also have other less obvious join constraints such as:

\begin{Verbatim}[fontsize=\scriptsize]
new-neighbor-pagerank(A, B, New),
neighbor-pagerank(A, B, Old)
   -o neighbor-pagerank(A, B, New).
\end{Verbatim}

Where variable \texttt{B} must be the same in both facts\footnote{Rule taken
from a asynchronous PageRank program.}.

\subsubsection{Iterators}

The data structures for facts presented in Section~\ref{sec:data_structures}
support the \emph{iterator} pattern. For linked lists, the iterator goes
through every fact in the list while the hash table iterator can either iterate
through the whole table or iterate through a single bucket. A bucket iterator is
a linked list iterator that is indexed by a given argument.
For tries, while the default iterator goes through every fact in
the trie, it can be customized with a matching specification in
order to reduce search. A matching specification includes argument
assignments (e.g., argument $i = V$, where $V$ is a concrete value).

Iterators are heavily used in compiled code. For instance, the second rule in
Fig.~\ref{code:shortest_path_program} is compiled as follows:

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("shortest").begin()); it1 != linked_list("shortest").end(); )
{
   fact *f1(*it1);
   for(auto it2(linked_list("relax").begin()); it2 != linked_list("relax").end(); )
   {
      fact *f2(*it2);

      if(f1->get_int(1) <= f2->get_int(1)) {
         fact *new_shortest(new fact("shortest"));
         new_shortest->set_int(1, f1->get_int(1));
         new_shortest->set_list(2, f1->get_list(2));

         // new fact was derived
         linked_list("shortest").push_back(new_shortest);

         // deleting facts
         fact::destroy(f1);
         it1 = linked_list("shortest").erase(it1);
         fact::destroy(f2);
         it2 = linked_list("relax").erase(it2);

         return;
      }
      ++it2;
   }
   ++it1;
}
\end{Verbatim}


The compilation algorithm iterates through the fact expressions in the body of
the rule and creates nested loops to try all the possible combinations of facts. For
this rule, all the pairs of facts \texttt{shortest} and
\texttt{relax} must be matched until the constraint \texttt{D1 <= D2} is
true. First, an iterator for \texttt{shortest} is created
that will loop through all \texttt{shortest} facts in the list. Inside the loop,
a nested iterator is created for predicate \texttt{relax}. This inner loop
includes a check for the \texttt{D1 <= D2} constraint. If the constraint expression is true then the rule
matches and a new \texttt{shortest} fact is derived and two used linear facts
are retracted by erasing the iterators from the linked lists. Note that after
the rule is derived, the code must return since there is a higher priority rule
that may the new \texttt{shortest} fact (see
Fig.~\ref{fig:shortest_path_program}. This enforces the priority semantics of
the language.
    
If the constraint had failed, another \texttt{relax} fact
would have been tried by incrementing \texttt{it2}. Likewise, the current
\texttt{f1} fact fails, another is used by incrementing \texttt{it1}.

\begin{figure}
\begin{algorithm}[H]
 \KwData{Rule R1, Rules}
 \KwResult{Compiled Code}
 $FactExprs \longleftarrow FactExprsFromRule(R1)$\;
 $Constraints \longleftarrow ConstraintsFromRule(R1)$\;
 $Function \longleftarrow CreateFunctionForRule()$\;
 $Code \longleftarrow Function$\;
 $Iterators \longleftarrow []$\;
 \While{$FactExprs$ not empty}{
  $Fact \longleftarrow RemoveBestFactExpr(FactExprs)$\;
  $Iterator \longleftarrow Code.InsertIterator(Fact)$\;
  $Iterators.push(Iterator)$\;
  $NextConstraints \longleftarrow RemoveNextConstraints(Constraints)$\;
  $Code \longleftarrow Code.CompileConstraints(NextConstraints)$\;
 }
 $Head = HeadTemplatesFromRule(R1)$\;
 \While{$Head$ not empty}{
    $Fact \longleftarrow RemoveNextFact(Head)$\;
    $Code.InsertDerivation(Fact)$\;
 }
 \For{$Iterator \in Iterators$}{
    \If{$IsLinear(Iterator)$}{
       $Code.AddRemove(Iterator)$\;
    }
 }
 \tcc{Enforce rule priorities}
 \If{$FactsDerivedUsedBefore(Head, Program, R1)$}{
    $Code.AddReturn()$\;
 }
 \Else{
    $Code.AddGoto(FirstLinear(Iterators))$\;
 }
 \Return{$Code$}
\end{algorithm}
 \caption{Compiling LM rules into C++ code.}
 \label{alg:compile_rule}
\end{figure}

Figure~\ref{alg:compile_rule} presents the for algorithm for compiling rules.
First we split the body of the rule into fact expressions and constraints. Fact
expressions map directly to iterators while fact constraints map to \emph{if}
expressions. A possible compilation strategy is to compile the fact expressions
and then only compile the constraints. However, this may require unneeded
database lookups since some constraints may fail early. Therefore, our compiler
introduces constraints as soon as all the variables in the constraint were
introduced by the already compiled fact expressions. The order in which fact
expressions are selected for compilation does not interfer with the correctness
of the compiled code, however our compiler selects the fact expressions with the
most potential activated constraints ($RemoveBestFactExpr$), therefore avoiding
undesirable database lookups. If two fact expressions have the same number of
new constraints, then the compiler always picks the persistent fact expression
since persistent facts are not deleted.

\subsubsection{Turning Linear Facts Into Persistent Facts}

Not all linear facts need to be deleted. For instance, in the compiled code
above, the fact \texttt{shortest(A, D1, P1)} is re-derived in the head. Our
compiler is able to turn linear loops into persistent loops for linear facts that are
retracted and then asserted.  The SSSP rule is then compiled as
follows:

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("shortest").begin()); it1 != linked_list("shortest").end(); )
{
   fact *f1(*it1);
   for(auto it2(linked_list("relax").begin()); it2 != linked_list("relax").end(); )
   {
      fact *f2(*it2);

      if(f1->get_int(1) <= f2->get_int(1)) {
         // deleting facts
         fact::destroy(f2);
         it2 = linked_list("relax").erase(it2);

         goto next;
      }
      ++it2;
next:
      continue;
   }
   ++it1;
}
\end{Verbatim}

Note that only the \texttt{relax} fact is deleted, while the \texttt{shortest}
facts are never deleted. In the SSSP program, each node has one
\texttt{shortest} fact and this compiled code simply filters out the
\texttt{relax} facts with the distances that are equal or greater
than the current best distance.

We now have a \emph{goto statement} (line 13) that is executed when the rule is fired.
In this case, since \texttt{shortest} is not retracted, the next \texttt{relax}
fact is used in order to fire rule as many times as possible. All the rule
combinations are attempted in cases where the rule does not derive any facts or
the facts derived do not appear before the rule, that is, the new facts are used in
lower priority rules. This is specified in the final \emph{if statement} in
Fig.~\ref{alg:compile_rule}. If the rule does not return, then we always jump to
the first loop that uses linear facts since we cannot use the next fact from the
deepest loop because we may have constraints between the first linear loop and
the deepest loop that were validated using deleted facts.

\subsubsection{Update Facts}

Many inference rules retract and then derive the same predicate but with
different arguments. The compiler recognizes those cases and instead of
retracting the fact from its linked list or hash table, it updates the fact
in-place. As an example, consider the following rule:

\begin{Verbatim}[fontsize=\scriptsize]
new-neighbor-pagerank(A, B, New),
neighbor-pagerank(A, B, Old)
   -o neighbor-pagerank(A, B, New).
\end{Verbatim}

Assuming \texttt{neighbor-rank} is stored in a hash table and indexed by the
second argument, the code for the rule above is as follows:

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("new-neighbor-pagerank").begin()); it1 !=
      linked_list("new-neighbor-pagerank").end(); )
{
   fact *f1(*it1);
   // hash table is indexed by the second argument
   // therefore we search for the bucket using the second argument of
   // new-neighbor-pagerank
   hash_bucket bucket(hash_table("neighbor-pagerank").find(f1->get_node(1));
   for(auto it2(bucket.begin()); it2 != bucket.end(); )
   {
      fact *f2(*it2);

      if(f1 != f2) {
         f2->set_float(2, f1->get_float(2)); // update neighbor-pagerank
         fact::destroy(f1);
         it1 = linked_list("new-neighbor-rank").erase(it1);

         goto next;
      }
      ++it2;
   }
   ++it1;
next:
   continue;
}
\end{Verbatim}

Note that \texttt{neighbor-pagerank} is updated using \texttt{set\_float}. The
rule also does not return since this is the highest priority rule. If there
was a higher priority rule using \texttt{neighbor-rank}, then the code
would have to return since an update represents a new fact.

\subsubsection{Enforcing Linearity}

We have already described the \emph{goto} statement compiled in each rule for avoiding
reusing retracted linear facts. However, this is not enough in order to enforce
linearity of facts. Consider the following inference rule:

\begin{Verbatim}
add(A, N1), add(A, N2) -o add(A, N1 + N2).
\end{Verbatim}

Using the standard compilation algorithm, two nested loops are created, one for
each \texttt{add} fact. However, notice that there is an implicit constraint
when creating the iterator for \texttt{add(A, N2)} since this fact cannot be the
same as the first one. That would invalidate linearity since a single linear fact would
be used to prove two linear facts. This is easily solved by adding a constraint
for the inner loop by checking if the fact pointer is the same as the first one.

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("add").begin()); it1 != linked_list("add").end(); )
{
   fact *f1(*it1);
   for(auto it2(linked_list("add").begin()); it2 != linked_list("add").end(); )
   {
      fact *f2(*it2);

      if(f1 != f2) {
         f1->set_int(1, f1->get_int(1) + f2->get_int(1));
         fact::destroy(f2);
         it2 = linked_list("add").erase(it2);

         goto next;
      }
      ++it2;
   }
   ++it1;
next:
   continue;
}
\end{Verbatim}

Figure~\ref{fig:update_add} presents the steps for executing this rule when the
database contains three facts. The iterators never point to the same fact.

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/update}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/update2}
\end{minipage}
\begin{minipage}{.5\textwidth}
   \centering
  \includegraphics[width=0.8\linewidth]{figures/update3}
\end{minipage}
\caption{Executing the add rule. First, the two iterators point to
   the first and second facts and the former is updated while the latter is
   retracted. The second iterator moves to the next fact and the first fact is
   updated again to \texttt{6}, the expected result.}
\label{fig:update_add}
\end{figure}

\subsubsection{Derivation}
Derivation of facts to the local node implies adding the new fact to the its
appropriate data structure. Facts that need to be sent to other nodes are sent
using an appropriate runtime API.

%\subsubsection{Merging Rules}

\subsection{Compiling Comprehensions}

Comprehensions were initially presented in the first rule of the SSSP program.
The rule is shown below:

\begin{Verbatim}[fontsize=\scriptsize]
shortest(A, D1, P1), D1 > D2, relax(A, D2, P2)
   -o shortest(A, D2, P2), {B, W | !edge(A, B, W) | relax(B, D2 + W, P2 ++ [B])}.
\end{Verbatim}

The attentive reader will remember that comprehensions are sub-rules, therefore
they should be compiled like normal rules. However, they do not need to return
due to rule priorities since all the combinations of the comprehension must be
derived. However, the rule itself must return if any of its comprehensions
have derived a fact that is used by a higher priority rule. For the example
rule, the rule does not need to return since it has the highest priority and the
\texttt{relax} fact derived in the comprehension is sent to another node.
The code for the rule is shown below:

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("shortest").begin()); it1 != linked_list("shortest").end(); )
{
   fact *f1(*it1);
   for(auto it2(linked_list("relax").begin()); it2 != linked_list("relax").end(); )
   {
      fact *f2(*it2);
      if(f1->get_int(1) > f2->get_int(1)) {
         // comprehension code
         for(auto it3(trie("edge").begin()); it3 != trie("edge").end(); ) {
            fact *f3(*it3);
            fact *new_relax(new fact("relax"));
            new_relax->set_int(1, f2->get_int(1) + f3->get_int(2));
            new_relax->set_list(append(f2->get_list(2), list(f3->get_node(1))));
            send_fact(f3, f3->get_node(1));
            ++it3;
         }

         fact::destroy(f2);
         it2 = linked_list("relax").erase(it2);

         f1->set_int(1, f2->get_int(1));
         f1->set_list(2, f2->get_list(2));
         goto next;
      }
      ++it2;
   }
   ++it1;
next:
   continue;
}
\end{Verbatim}

Special care must be taken when the comprehension's sub-rule uses the same
predicates that are derived by the main rule.
Rule inference must be atomic in the sense that after a rule matches, the
comprehensions in the head of the rule can use the facts that were present
before the body of the rule was matched.
Consider a rule with $n$ comprehensions or aggregates, where $CB_i$ and $CH_i$
is the body and head of the comprehension/aggregate, respectively, and $H$
represents the fact templates found in the head of the rule.
The formula used by the compiler to detect conflicts between predicates is the
following:

\[
\bigcap^{n}_i[CB_i \cap H] \cup \bigcap^{n}_i [CB_i \cap \bigcup^{n}_j[CH_j]]
\]

If the result of the formula is not empty, then the compiler disables
optimizations for the conflicting predicates and derives the corresponding facts
into the fact buffer that are then added back into the database.
Fortunately, most rules in LM programs do not have conflicts and thus
are fully optimized.

\subsection{Compiling Aggregates}

Aggregates are similar to comprehensions. They are also sub-rules but a value is
accumulated for each combination of the sub-rule. After all the combinations are
inferred, a final head term is derived with the accumulated term. Consider the following
PageRank rule:

\begin{Verbatim}[fontsize=\scriptsize]
update(A), pagerank(A, OldRank)
      -o [sum => V | B | neighbor-pagerank(A, B, V) | neighbor-pagerank(A, B, V) |
            pagerank(A, damp/P + (1.0 - damp) * V)].
\end{Verbatim}

The variable \texttt{V} is is initialized to \texttt{0.0} and accumulates all
the PageRank values of the neighbors as seen in the code below. Since the
\texttt{pagerank} fact can be updated, its second argument is calculated using
the aggregated value.

\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
for(auto it1(linked_list("pagerank").begin()); it1 != linked_list("pagerank").end(); )
{
   fact *f1(*it1);
   for(auto it2(linked_list("update").begin()); it2 != linked_list("update").end(); )
   {
      fact *f2(*it2);

      double acc(0.0); // aggregate accumulator.
      for(auto it3(linked_list("neighbor-pagerank").begin()); it3 !=
            linked_list("neighbor-pagerank").end(); ) {
         fact *f3(*it3);

         acc += f3->get_float(2);
         ++it3; // the sub-rule has no head since neighbor-pagerank is re-derived
      }
      // head of the aggregate
      f1->set_float(1, damp / P + (1.0 - damp) * V);
      goto next;
   }
   ++it1;
next:
   continue;
}
\end{Verbatim}
